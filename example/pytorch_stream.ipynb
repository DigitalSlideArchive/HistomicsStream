{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f87613a9",
   "metadata": {},
   "source": [
    "# Demonstration of histomics_stream\n",
    "\n",
    "Click to open in [[GitHub](https://github.com/DigitalSlideArchive/HistomicsStream/tree/master/example/pytorch.ipynb)] [[Google Colab](https://colab.research.google.com/github/DigitalSlideArchive/HistomicsStream/blob/master/example/pytorch_stream.ipynb)]\n",
    "\n",
    "The `histomics_stream` Python package sits at the start of any machine learning workflow that is built on the PyTorch machine learning library.  The package is responsible for efficient access to the input image data that will be used to fit a new machine learning model or will be used to predict regions of interest in novel inputs using an already learned model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8490f25",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "If you are running this notebook on Google Colab or another system where `histomics_stream` and its dependencies are not yet installed then they can be installed with the following commands.  Note that image readers in addition to openslide are also supported by using, e.g., `large_image[bioformats,ometiff,openjpeg,openslide,tiff]` on the below pip install command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa174fa-c59b-42a5-ae59-7b28d3b3c50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get histomics_stream and its dependencies\n",
    "!apt update\n",
    "!apt install -y python3-openslide openslide-tools\n",
    "!pip install 'large_image[openslide,tiff]' --find-links https://girder.github.io/large_image_wheels\n",
    "!pip install histomics_stream\n",
    "\n",
    "# Get other packages used in this notebook\n",
    "# N.B. itkwidgets works with jupyter<=3.0.0\n",
    "!apt install libcudnn8 libcudnn8-dev\n",
    "!pip install pooch itkwidgets\n",
    "!jupyter labextension install @jupyter-widgets/jupyterlab-manager jupyter-matplotlib jupyterlab-datawidgets itkwidgets\n",
    "\n",
    "print(\n",
    "    \"\\nNOTE!: On Google Colab you may need to choose 'Runtime->Restart runtime' for these updates to take effect.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2efd66",
   "metadata": {},
   "source": [
    "## Fetching and creating the test data\n",
    "This notebook has demonstrations that use the files `TCGA-AN-A0G0-01Z-00-DX1.svs` (365 MB) and `TCGA-AN-A0G0-01Z-00-DX1.mask.png` (4 kB),  The pooch commands will fetch them if they are not already available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ea3c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pooch\n",
    "\n",
    "# download whole slide image\n",
    "wsi_path = pooch.retrieve(\n",
    "    fname=\"TCGA-AN-A0G0-01Z-00-DX1.svs\",\n",
    "    url=\"https://drive.google.com/uc?export=download&id=19agE_0cWY582szhOVxp9h3kozRfB4CvV&confirm=t&uuid=6f2d51e7-9366-4e98-abc7-4f77427dd02c&at=ALgDtswlqJJw1KU7P3Z1tZNcE01I:1679111148632\",\n",
    "    known_hash=\"d046f952759ff6987374786768fc588740eef1e54e4e295a684f3bd356c8528f\",\n",
    "    path=str(pooch.os_cache(\"pooch\")) + os.sep + \"wsi\",\n",
    ")\n",
    "print(f\"Have {wsi_path}\")\n",
    "\n",
    "# download binary mask image\n",
    "mask_path = pooch.retrieve(\n",
    "    fname=\"TCGA-AN-A0G0-01Z-00-DX1.mask.png\",\n",
    "    url=\"https://drive.google.com/uc?export=download&id=17GOOHbL8Bo3933rdIui82akr7stbRfta\",\n",
    "    known_hash=\"bb657ead9fd3b8284db6ecc1ca8a1efa57a0e9fd73d2ea63ce6053fbd3d65171\",\n",
    "    path=str(pooch.os_cache(\"pooch\")) + os.sep + \"wsi\",\n",
    ")\n",
    "print(f\"Have {mask_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4274b5d6",
   "metadata": {},
   "source": [
    "## Creating a study for use with histomics_stream\n",
    "\n",
    "We describe the input and desired parameters using standard Python lists and dictionaries.  Here we give a high-level configuration; selection of tiles is done subsequently.\n",
    "\n",
    "N.B.: __*all*__ values that are number of pixels are based upon the `target_magnification` that is supplied to `FindResolutionForSlide`.  This includes pixel sizes of a slide, chunk, or tile and it includes the pixel coordinates for a chunk or tile.  It applies whether the numbers are supplied to histomics_stream or returned by histomics_stream.  However, if the `magnification_source` is not `exact` the `returned_magnification` may not equal the `target_magnification`; to get the number of pixels that is relevant for the `returned_magnification`, typically these numbers of pixels are multiplied by the ratio `returned_magnification / target_magnification`.  In particular, the *pixel size of the returned tiles* will be the requested size times this ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e17612d-0216-4652-92cd-d8ea5e0ac6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import histomics_stream as hs\n",
    "import histomics_stream.pytorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effa3803-fc82-4bd2-93f1-538de00d7607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a study and insert study-wide information.\n",
    "# Add a slide to the study, including slide-wide information with it.\n",
    "my_study0 = dict(\n",
    "    version=\"version-1\",\n",
    "    tile_height=256,\n",
    "    tile_width=256,\n",
    "    overlap_height=0,\n",
    "    overlap_width=0,\n",
    "    slides=dict(\n",
    "        Slide_0=dict(\n",
    "            filename=wsi_path,\n",
    "            slide_name=os.path.splitext(os.path.split(wsi_path)[1])[0],\n",
    "            slide_group=\"Group 3\",\n",
    "            chunk_height=2048,\n",
    "            chunk_width=2048,\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "# For each slide, find the appropriate resolution given the target_magnification and\n",
    "# magnification_tolerance.  In this example, we use the same parameters for each slide,\n",
    "# but this is not required generally.\n",
    "find_slide_resolution = hs.configure.FindResolutionForSlide(\n",
    "    my_study0, target_magnification=20, magnification_source=\"exact\"\n",
    ")\n",
    "for slide in my_study0[\"slides\"].values():\n",
    "    find_slide_resolution(slide)\n",
    "print(f\"my_study0 = {my_study0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fde9d2e",
   "metadata": {},
   "source": [
    "## Tile selection\n",
    "\n",
    "We are going to demonstrate several approaches to choosing tiles.  Each approach will start with its own copy of the `my_study0` that we have built so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca79608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba3ab43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate TilesByGridAndMask without a mask\n",
    "my_study_by_grid = copy.deepcopy(my_study0)\n",
    "tiles_by_grid = hs.configure.TilesByGridAndMask(\n",
    "    my_study_by_grid, overlap_height=32, overlap_width=32, randomly_select=5\n",
    ")\n",
    "# We could apply this to a subset of the slides, but we will apply it to all slides in\n",
    "# this example.\n",
    "for slide in my_study_by_grid[\"slides\"].values():\n",
    "    tiles_by_grid(slide)\n",
    "# Take a look at what we have made\n",
    "print(f\"==== The entire dictionary is now ==== \\nmy_study_by_grid = {my_study_by_grid}\")\n",
    "just_tiles = tiles_by_grid.get_tiles(my_study_by_grid)\n",
    "print(f\"==== A quick look at just the tiles is now ====\\njust_tiles = {just_tiles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953ebb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate TilesByGridAndMask with a mask\n",
    "my_study_by_grid_and_mask = copy.deepcopy(my_study0)\n",
    "tiles_by_grid_and_mask = hs.configure.TilesByGridAndMask(\n",
    "    my_study_by_grid_and_mask, mask_filename=mask_path, randomly_select=10\n",
    ")\n",
    "# We could apply this to a subset of the slides, but we will apply it to all slides in\n",
    "# this example.\n",
    "for slide in my_study_by_grid_and_mask[\"slides\"].values():\n",
    "    tiles_by_grid_and_mask(slide)\n",
    "# Take a look at what we have made\n",
    "print(\n",
    "    f\"==== The entire dictionary is now ==== \\nmy_study_by_grid_and_mask = {my_study_by_grid_and_mask}\"\n",
    ")\n",
    "just_tiles = tiles_by_grid_and_mask.get_tiles(my_study_by_grid_and_mask)\n",
    "print(f\"==== A quick look at just the tiles is now ====\\njust_tiles = {just_tiles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f341e882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate TilesByList\n",
    "my_study_by_list = copy.deepcopy(my_study0)\n",
    "tiles_by_list = hs.configure.TilesByList(\n",
    "    my_study_by_list,\n",
    "    randomly_select=5,\n",
    "    tiles_dictionary=my_study_by_grid[\"slides\"][\"Slide_0\"][\"tiles\"],\n",
    ")\n",
    "# We could apply this to a subset of the slides, but we will apply it to all slides in\n",
    "# this example.\n",
    "for slide in my_study_by_list[\"slides\"].values():\n",
    "    tiles_by_list(slide)\n",
    "# Take a look at what we have made\n",
    "print(f\"==== The entire dictionary is now ==== \\nmy_study_by_list = {my_study_by_list}\")\n",
    "just_tiles = tiles_by_list.get_tiles(my_study_by_list)\n",
    "print(f\"==== A quick look at just the tiles is now ====\\njust_tiles = {just_tiles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc2770f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate TilesRandomly\n",
    "my_study_randomly = copy.deepcopy(my_study0)\n",
    "tiles_randomly = hs.configure.TilesRandomly(my_study_randomly, randomly_select=10)\n",
    "# We could apply this to a subset of the slides, but we will apply it to all slides in\n",
    "# this example.\n",
    "for slide in my_study_randomly[\"slides\"].values():\n",
    "    tiles_randomly(slide)\n",
    "# Take a look at what we have made\n",
    "print(\n",
    "    f\"==== The entire dictionary is now ==== \\nmy_study_randomly = {my_study_randomly}\"\n",
    ")\n",
    "just_tiles = tiles_randomly.get_tiles(my_study_randomly)\n",
    "print(f\"==== A quick look at just the tiles is now ====\\njust_tiles = {just_tiles}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35fe040",
   "metadata": {},
   "source": [
    "## Creating a Dataset\n",
    "\n",
    "We request tiles indicated by the mask and create a Dataset that has the image data for these tiles as well as associated parameters for each tile, such as its location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d272866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate TilesByGridAndMask with a mask\n",
    "my_study = copy.deepcopy(my_study0)\n",
    "tiles_by_grid_and_mask = hs.configure.TilesByGridAndMask(\n",
    "    my_study, mask_filename=mask_path, mask_threshold=0.5, randomly_select=100\n",
    ")\n",
    "for slide in my_study[\"slides\"].values():\n",
    "    tiles_by_grid_and_mask(slide)\n",
    "print(\"Finished selecting tiles.\")\n",
    "\n",
    "create_pytorch_dataloader = hs.pytorch.CreateTorchDataloader()\n",
    "tiles = create_pytorch_dataloader(my_study)\n",
    "print(\"Finished with CreateTorchDataloader\")\n",
    "# print(f\"{tile = }\")\n",
    "# print(f\"... with tile shape = {tiles.take(1).get_single_element()[0][0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800f2502",
   "metadata": {},
   "source": [
    "## Fetch a model for prediction\n",
    "\n",
    "We build a arbitrary but reasonable model for demonstration purposes.\n",
    "\n",
    "Because each element of our Dataset is a tuple `(rgb_image_data, dictionary_of_annotation)`, a typical model that accepts only the former as its input needs to be wrapped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd890cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTorchModel(torch.nn.modules.module.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, tile_height, tile_width, num_categories, kernel_size\n",
    "    ):\n",
    "        print(f\"{in_channels = }\")\n",
    "        print(f\"{tile_height = }\")\n",
    "        print(f\"{tile_width = }\")\n",
    "        print(f\"{num_categories = }\")\n",
    "        print(f\"{kernel_size = }\")\n",
    "        super(MyTorchModel, self).__init__()\n",
    "        out1_channels = 2 * in_channels\n",
    "        padding = tuple(int((k - 1) // 2) for k in kernel_size)\n",
    "        self.conv1 = torch.nn.Conv2d(\n",
    "            in_channels, out1_channels, kernel_size, padding=padding\n",
    "        )\n",
    "        out2_channels = 4 * in_channels\n",
    "        self.conv2 = torch.nn.Conv2d(\n",
    "            out1_channels, out2_channels, kernel_size, padding=padding\n",
    "        )\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.pool = torch.nn.MaxPool2d(2, 2)\n",
    "        self.flat_size = int(\n",
    "            in_channels * tile_height * tile_width / (out2_channels / in_channels)\n",
    "        )\n",
    "        self.fc1 = torch.nn.Linear(self.flat_size, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, num_categories)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, self.flat_size)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "unwrapped_model = MyTorchModel(\n",
    "    in_channels=3,\n",
    "    tile_height=my_study_randomly[\"tile_height\"],\n",
    "    tile_width=my_study_randomly[\"tile_width\"],\n",
    "    num_categories=2,\n",
    "    kernel_size=(5, 5),\n",
    ")\n",
    "\n",
    "# At this point it would be standard to train the model.  This example is so dumb that\n",
    "# we won't do that here.\n",
    "\n",
    "\n",
    "class WrapModel(torch.nn.modules.module.Module):\n",
    "    def __init__(self, model, *args, **kwargs):\n",
    "        super(WrapModel, self).__init__(*args, **kwargs)\n",
    "        self.model = unwrapped_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.model(x[0])\n",
    "        return p, x[1]\n",
    "\n",
    "\n",
    "model = WrapModel(unwrapped_model)\n",
    "print(\"Model created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e687409",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e890c9-9400-4324-ba6d-22d3aae90669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"Starting predictions\")\n",
    "start_time = time.time()\n",
    "# Consider adding a batch factor to the data loader\n",
    "predictions = [model(tile) for tile in tiles]\n",
    "end_time = time.time()\n",
    "print(\"Done predicting\")\n",
    "num_inputs = len([0 for tile in tiles])\n",
    "num_predictions = len(predictions)\n",
    "print(\n",
    "    f\"Made {num_predictions} predictions for {num_inputs} tiles \"\n",
    "    f\"in {end_time - start_time} s.\"\n",
    ")\n",
    "print(f\"Average of {(end_time - start_time) / num_inputs} s per tile.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f16b044",
   "metadata": {},
   "source": [
    "## Look at internals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c277a4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_iter = iter(tiles)\n",
    "tile = next(tile_iter)\n",
    "print(f\"                       {type(tiles) = }\")\n",
    "print(f\"               {type(tiles.dataset) = }\")\n",
    "print(f\"         {type(iter(tiles.dataset)) = }\")\n",
    "print(f\"                   {type(tile_iter) = }\")\n",
    "print(f\"                        {type(tile) = }\")\n",
    "print(f\"                         {len(tile) = }\")\n",
    "print(f\"                     {type(tile[0]) = }\")\n",
    "print(f\"                     {tile[0].shape = }\")\n",
    "print(f\"                     {type(tile[1]) = }\")\n",
    "print(f\"{tile[0][0,0,0,0].to(torch.float32) = }\")\n",
    "pred = predictions[0]\n",
    "print(f\"                 {type(predictions) = }\")\n",
    "print(f\"                  {len(predictions) = }\")\n",
    "print(f\"                        {type(pred) = }\")\n",
    "print(f\"                         {len(pred) = }\")\n",
    "print(f\"                     {type(pred[0]) = }\")\n",
    "print(f\"                     {pred[0].shape = }\")\n",
    "print(f\"                           {pred[0] = }\")\n",
    "print(f\"                     {type(pred[1]) = }\")\n",
    "print(f\"                    {pred[1].keys() = }\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
